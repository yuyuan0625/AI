{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMKXlqEdruJkJZqLe15Uox9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Pjf0ydnVs24O"},"source":["# Chapter2\n","## Perception, Logistic Regerssion, Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"xpCUvzQ6ZgXX"},"source":["### 建模型前先設定一些操參數\n","\n","\n","1.   資料分成多少batch\n","2.   丟進去的feature數\n","3.   return的feature數\n","4.   epoch number:做幾個iteration\n","5.   learning rate\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"AYaQ_1_crGQ4"},"source":["batch_n = 100\n","in_features = 3\n","out_features = 1\n","epoch_n = 100\n","learning_rate = 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LOI3iA7Uc9Tp"},"source":["### 目標:線性回歸"]},{"cell_type":"code","metadata":{"id":"jT9kuYotdk6Q"},"source":["import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WVel0U5ScrCE"},"source":["#假設 y = 3x_1 + 5x_2 + 1x_3\n","x = torch.rand(batch_n, in_features) #訓練模型的資料(100筆,3欄)\n","c = torch.Tensor([[3.], [5.], [1.]]) #vector 但要排成直的才能和x相乘->做成二維陣列\n","y = x.mm(c) #matrix multiply\n","#資料量少，太容易train->增加雜訊\n","y = y.add(torch.rand(batch_n, out_features))\n","y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YA_OYb1rfvYA"},"source":["### 建立模型\n","#### 資料分析: 畫散佈圖"]},{"cell_type":"code","metadata":{"id":"8x5aHXA9ftm_"},"source":["import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"dI6vLZQkgE4Q","executionInfo":{"status":"ok","timestamp":1632989777569,"user_tz":-480,"elapsed":440,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"abb55b27-1d49-4da0-c4b0-1965d24c37bc"},"source":["plt.figure() #先建立figure物件\n","plt.scatter(x[:,1], y)  #取x1時會發現較集中(共線性高),因為係數為5\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rT4HruAXhWIq","executionInfo":{"status":"ok","timestamp":1632989777570,"user_tz":-480,"elapsed":21,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"cedb2de8-f060-4ef3-c686-f6c89f4aa68e"},"source":["w = torch.rand(in_features, out_features) #目標:藉由train讓w接近3,5,1\n","w"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.7417],\n","        [0.4617],\n","        [0.5818]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"3JM0yExohb7e"},"source":["#### Gradient Descent\n","1. 先把資料丟到模型內，得到預測結果y^\n","\n","2. y^和y算loss function\n","\n","3. 計算gradient\n","\n","4. update參數"]},{"cell_type":"markdown","metadata":{"id":"Y7pebPEWwVyV"},"source":["### 土法煉鋼(有時候辦法用pytorch套件)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LiSRVHMVhRQ_","executionInfo":{"status":"ok","timestamp":1632989777571,"user_tz":-480,"elapsed":16,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"9a47e80f-259c-4ca1-a5f7-65c2089bc198"},"source":["for epoch in range(epoch_n):\n","  y_pred = x.mm(w)\n","  loss = (y_pred-y).pow(2).sum() #mse，取sum和取avg差不多\n","  print(\"Epoch:{}, Loss:{:.4f}\".format(epoch,loss))\n","\n","  grad_y_pred = 2 * (y_pred-y) #(100x1)的vector\n","  grad_w = x.t().mm(grad_y_pred) #x為(100x3)->轉置成(3x100)\n","  #(3x100)x(100x1)->目標3x1\n","  \n","  w -= learning_rate * grad_w #update w\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:0, Loss:1728.7399\n","Epoch:1, Loss:1270.3756\n","Epoch:2, Loss:937.5721\n","Epoch:3, Loss:695.8423\n","Epoch:4, Loss:520.1736\n","Epoch:5, Loss:392.4255\n","Epoch:6, Loss:299.4416\n","Epoch:7, Loss:231.6796\n","Epoch:8, Loss:182.2192\n","Epoch:9, Loss:146.0407\n","Epoch:10, Loss:119.5038\n","Epoch:11, Loss:99.9677\n","Epoch:12, Loss:85.5172\n","Epoch:13, Loss:74.7631\n","Epoch:14, Loss:66.6974\n","Epoch:15, Loss:60.5890\n","Epoch:16, Loss:55.9076\n","Epoch:17, Loss:52.2680\n","Epoch:18, Loss:49.3910\n","Epoch:19, Loss:47.0738\n","Epoch:20, Loss:45.1690\n","Epoch:21, Loss:43.5696\n","Epoch:22, Loss:42.1979\n","Epoch:23, Loss:40.9973\n","Epoch:24, Loss:39.9266\n","Epoch:25, Loss:38.9557\n","Epoch:26, Loss:38.0626\n","Epoch:27, Loss:37.2313\n","Epoch:28, Loss:36.4499\n","Epoch:29, Loss:35.7098\n","Epoch:30, Loss:35.0044\n","Epoch:31, Loss:34.3290\n","Epoch:32, Loss:33.6800\n","Epoch:33, Loss:33.0544\n","Epoch:34, Loss:32.4503\n","Epoch:35, Loss:31.8659\n","Epoch:36, Loss:31.2999\n","Epoch:37, Loss:30.7512\n","Epoch:38, Loss:30.2190\n","Epoch:39, Loss:29.7024\n","Epoch:40, Loss:29.2009\n","Epoch:41, Loss:28.7137\n","Epoch:42, Loss:28.2405\n","Epoch:43, Loss:27.7807\n","Epoch:44, Loss:27.3338\n","Epoch:45, Loss:26.8996\n","Epoch:46, Loss:26.4775\n","Epoch:47, Loss:26.0673\n","Epoch:48, Loss:25.6686\n","Epoch:49, Loss:25.2809\n","Epoch:50, Loss:24.9041\n","Epoch:51, Loss:24.5378\n","Epoch:52, Loss:24.1817\n","Epoch:53, Loss:23.8355\n","Epoch:54, Loss:23.4989\n","Epoch:55, Loss:23.1717\n","Epoch:56, Loss:22.8535\n","Epoch:57, Loss:22.5442\n","Epoch:58, Loss:22.2435\n","Epoch:59, Loss:21.9511\n","Epoch:60, Loss:21.6668\n","Epoch:61, Loss:21.3904\n","Epoch:62, Loss:21.1217\n","Epoch:63, Loss:20.8604\n","Epoch:64, Loss:20.6063\n","Epoch:65, Loss:20.3592\n","Epoch:66, Loss:20.1190\n","Epoch:67, Loss:19.8854\n","Epoch:68, Loss:19.6583\n","Epoch:69, Loss:19.4375\n","Epoch:70, Loss:19.2227\n","Epoch:71, Loss:19.0139\n","Epoch:72, Loss:18.8108\n","Epoch:73, Loss:18.6133\n","Epoch:74, Loss:18.4213\n","Epoch:75, Loss:18.2345\n","Epoch:76, Loss:18.0529\n","Epoch:77, Loss:17.8764\n","Epoch:78, Loss:17.7046\n","Epoch:79, Loss:17.5376\n","Epoch:80, Loss:17.3752\n","Epoch:81, Loss:17.2173\n","Epoch:82, Loss:17.0636\n","Epoch:83, Loss:16.9143\n","Epoch:84, Loss:16.7690\n","Epoch:85, Loss:16.6277\n","Epoch:86, Loss:16.4903\n","Epoch:87, Loss:16.3566\n","Epoch:88, Loss:16.2267\n","Epoch:89, Loss:16.1002\n","Epoch:90, Loss:15.9773\n","Epoch:91, Loss:15.8577\n","Epoch:92, Loss:15.7414\n","Epoch:93, Loss:15.6283\n","Epoch:94, Loss:15.5183\n","Epoch:95, Loss:15.4113\n","Epoch:96, Loss:15.3072\n","Epoch:97, Loss:15.2059\n","Epoch:98, Loss:15.1075\n","Epoch:99, Loss:15.0117\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xo9imRdPvlFP","executionInfo":{"status":"ok","timestamp":1632989777572,"user_tz":-480,"elapsed":13,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"3b3fe25b-72c7-42fa-bb7f-1ce7ca1b1074"},"source":["w #因為在y中有加雜訊，所以不會是3,5,1\n","#係數位差最大的會是細數最小的那欄->共線性越高，受雜訊影響越少;共線性越低，受雜訊影響越多"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[3.2706],\n","        [4.7959],\n","        [1.8659]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"ah5GlAVNwjRA"},"source":["### 用pytorch套件 Auto-grad\n","#### 是利用數值方法去逼近，所以當模型複雜時誤差會變大->用手算偏微分(土法煉鋼)可能會較準"]},{"cell_type":"code","metadata":{"id":"AQNNN_h3vV4A"},"source":["from torch.autograd import Variable"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r7YtRtw3w3J1"},"source":["Vx = Variable(x, requires_grad=False) #x tenser放到Variable，因為不用算x的grad->requires_grad=False\n","Vy = Variable(y, requires_grad=False)\n","Vw = Variable(torch.rand(in_features, out_features), requires_grad=True) \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBmB7GrIxyVs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632989777877,"user_tz":-480,"elapsed":312,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"5412615d-be74-4418-c799-181cbb433ad0"},"source":["for epoch in range(epoch_n):\n","  y_pred = Vx.mm(Vw)\n","  loss = (y_pred-Vy).pow(2).sum() #mse，取sum和取avg差不多\n","  print(\"Epoch:{}, Loss:{:.4f}\".format(epoch,loss))\n","\n","  loss.backward() #會自動把可以計算gradient的都算好\n","  \n","  Vw.data -= learning_rate *Vw.grad.data  #grad放在Variable物件的.grad.data\n","\n","  Vw.grad.data.zero_()  #做loss.backword時會把grad累加進去，值會有問題->每個loop要歸零\n","  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:0, Loss:1788.9257\n","Epoch:1, Loss:1312.3743\n","Epoch:2, Loss:966.4219\n","Epoch:3, Loss:715.1953\n","Epoch:4, Loss:532.6779\n","Epoch:5, Loss:400.0009\n","Epoch:6, Loss:303.4792\n","Epoch:7, Loss:233.1877\n","Epoch:8, Loss:181.9279\n","Epoch:9, Loss:144.4790\n","Epoch:10, Loss:117.0541\n","Epoch:11, Loss:96.9069\n","Epoch:12, Loss:82.0452\n","Epoch:13, Loss:71.0240\n","Epoch:14, Loss:62.7951\n","Epoch:15, Loss:56.5981\n","Epoch:16, Loss:51.8814\n","Epoch:17, Loss:48.2445\n","Epoch:18, Loss:45.3971\n","Epoch:19, Loss:43.1282\n","Epoch:20, Loss:41.2848\n","Epoch:21, Loss:39.7556\n","Epoch:22, Loss:38.4598\n","Epoch:23, Loss:37.3386\n","Epoch:24, Loss:36.3493\n","Epoch:25, Loss:35.4606\n","Epoch:26, Loss:34.6498\n","Epoch:27, Loss:33.9003\n","Epoch:28, Loss:33.1998\n","Epoch:29, Loss:32.5394\n","Epoch:30, Loss:31.9123\n","Epoch:31, Loss:31.3137\n","Epoch:32, Loss:30.7398\n","Epoch:33, Loss:30.1878\n","Epoch:34, Loss:29.6556\n","Epoch:35, Loss:29.1415\n","Epoch:36, Loss:28.6441\n","Epoch:37, Loss:28.1623\n","Epoch:38, Loss:27.6955\n","Epoch:39, Loss:27.2426\n","Epoch:40, Loss:26.8033\n","Epoch:41, Loss:26.3768\n","Epoch:42, Loss:25.9628\n","Epoch:43, Loss:25.5607\n","Epoch:44, Loss:25.1702\n","Epoch:45, Loss:24.7908\n","Epoch:46, Loss:24.4223\n","Epoch:47, Loss:24.0643\n","Epoch:48, Loss:23.7164\n","Epoch:49, Loss:23.3785\n","Epoch:50, Loss:23.0500\n","Epoch:51, Loss:22.7309\n","Epoch:52, Loss:22.4208\n","Epoch:53, Loss:22.1195\n","Epoch:54, Loss:21.8266\n","Epoch:55, Loss:21.5421\n","Epoch:56, Loss:21.2655\n","Epoch:57, Loss:20.9967\n","Epoch:58, Loss:20.7355\n","Epoch:59, Loss:20.4816\n","Epoch:60, Loss:20.2349\n","Epoch:61, Loss:19.9951\n","Epoch:62, Loss:19.7620\n","Epoch:63, Loss:19.5355\n","Epoch:64, Loss:19.3154\n","Epoch:65, Loss:19.1014\n","Epoch:66, Loss:18.8934\n","Epoch:67, Loss:18.6912\n","Epoch:68, Loss:18.4947\n","Epoch:69, Loss:18.3037\n","Epoch:70, Loss:18.1180\n","Epoch:71, Loss:17.9375\n","Epoch:72, Loss:17.7621\n","Epoch:73, Loss:17.5915\n","Epoch:74, Loss:17.4258\n","Epoch:75, Loss:17.2646\n","Epoch:76, Loss:17.1079\n","Epoch:77, Loss:16.9557\n","Epoch:78, Loss:16.8076\n","Epoch:79, Loss:16.6637\n","Epoch:80, Loss:16.5238\n","Epoch:81, Loss:16.3877\n","Epoch:82, Loss:16.2555\n","Epoch:83, Loss:16.1270\n","Epoch:84, Loss:16.0020\n","Epoch:85, Loss:15.8804\n","Epoch:86, Loss:15.7623\n","Epoch:87, Loss:15.6474\n","Epoch:88, Loss:15.5358\n","Epoch:89, Loss:15.4272\n","Epoch:90, Loss:15.3216\n","Epoch:91, Loss:15.2190\n","Epoch:92, Loss:15.1192\n","Epoch:93, Loss:15.0221\n","Epoch:94, Loss:14.9278\n","Epoch:95, Loss:14.8360\n","Epoch:96, Loss:14.7468\n","Epoch:97, Loss:14.6601\n","Epoch:98, Loss:14.5758\n","Epoch:99, Loss:14.4938\n"]}]},{"cell_type":"code","metadata":{"id":"DmN_5D9izoir","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632989777877,"user_tz":-480,"elapsed":18,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"5c953476-272b-40e8-ca42-cc6bc872b9c4"},"source":["Vw.data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[3.2915],\n","        [4.8242],\n","        [1.8150]])"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"rbpful4F0ZHm"},"source":["###Torch nn.Module 將模型包成一個類別，宣告成物件\n","#### 複雜模型會有多個Variable，不可能宣告那麼多variable"]},{"cell_type":"code","metadata":{"id":"G8Y43kgM0Lqk"},"source":["import torch.nn as nn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o1ninySE08Ea"},"source":["class Model(nn.Module): #類別名稱第一個字大寫，()繼承所有傳統的類神經; \n","  def __init__(self): #有些東西會寫成抽象類別，因為一開始不知道模型架構。任何類別都會有Constructor，若要用父類別建構好的東西，要在自己程式中呼叫父類別Constructor\n","    super(Model, self).__init__()\n","  def forward(self, x, w): #model如何建構起來\n","    y_pred = x.mm(w)\n","    return y_pred\n","  def backward(self): #已被實作->pass\n","    pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GG4GA7wk3y-m"},"source":["modle = Model()\n","Vw = Variable(torch.rand(in_features, out_features), requires_grad=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9kDBsVs5hHu"},"source":["for epoch in range(epoch_n):\n","  y_pred = modle(Vx,Vw)\n","  loss = (y_pred-Vy).pow(2).sum() #mse，取sum和取avg差不多\n","  print(\"Epoch:{}, Loss:{:.4f}\".format(epoch,loss))\n","\n","  loss.backward() #會自動把可以計算gradient的都算好\n","  \n","  Vw.data -= learning_rate *Vw.grad.data  #grad放在Variable物件的.grad.data\n","\n","  Vw.grad.data.zero_()  #做loss.backword時會把grad累加進去，值會有問題->每個loop要歸零\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1JQNO4DD6SlR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632989777879,"user_tz":-480,"elapsed":12,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"49c19c1e-6d46-4788-83a5-4c034fa51f62"},"source":["Vw.data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[3.2019],\n","        [4.8477],\n","        [1.8831]])"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"gtRBxrVI6ngU"},"source":["### loss function"]},{"cell_type":"code","metadata":{"id":"Q33tpuv26mq8"},"source":["#使用torch.nn提供的layer\n","class Model(nn.Module): #類別名稱第一個字大寫，()繼承所有傳統的類神經; \n","  def __init__(self): #有些東西會寫成抽象類別，因為一開始不知道模型架構。任何類別都會有Constructor，若要用父類別建構好的東西，要在自己程式中呼叫父類別Constructor\n","    super(Model, self).__init__()\n","    self.linear = nn.Linear(in_features, out_features, False) #nn.Linear建一個類別叫self.linear; False:不要bias項(常數項)\n","  def forward(self, x): #不需要w了，因為用內建loss function\n","    y_pred = self.linear(x)\n","    return y_pred\n","  def backward(self): #已被實作->pass\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHY54yRdSVp7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632989777880,"user_tz":-480,"elapsed":10,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"7f40f2b4-44d0-4c75-fcb4-d20552a94fc1"},"source":["model = Model()\n","loss_fn = nn.MSELoss() #loss function\n","print(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model(\n","  (linear): Linear(in_features=3, out_features=1, bias=False)\n",")\n"]}]},{"cell_type":"code","metadata":{"id":"k5TfU4wATEvp"},"source":["for epoch in range(epoch_n*100):  #下面loss是取平均過後，所以loss是上面的1/100->收斂速度變慢->epoch_n*100\n","  y_pred = model(x)\n","  loss = loss_fn(y_pred, y) #(預測結果,實際答案)\n","  if (epoch+1)%100 == 0: #每100次輸出一次loss\n","    print(\"Epoch:{}, Loss:{:.4f}\".format(epoch,loss))\n","  model.zero_grad() #計算gradient前把gradient歸零，因為linear layer內會先隨機給gradient一個initial值，所以要先清除才不會影響backward\n","  loss.backward() #會自動把可以計算gradient的都算好\n","  for param in model.parameters(): #每層參數放到list中->用for取 (現在只有一層->可寫model.parameters(0))\n","    param.data -= learning_rate*param.grad.data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cYJrw7lVYB9S"},"source":[" for param in model.parameters(): \n","    print(param.data) #param為一個object"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NBuVYWk6YWJN"},"source":["model = Model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oT8LdqYpfSbj","executionInfo":{"status":"ok","timestamp":1632989780053,"user_tz":-480,"elapsed":10,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"e3e69be4-1534-4857-ca80-1bcff8ecf5a0"},"source":[" for param in model.parameters(): #確認是否是新的\n","    print(param.data)  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.2874, -0.3100, -0.1515]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"mSxzvmqgg93H"},"source":["### optimizier"]},{"cell_type":"code","metadata":{"id":"DvtbDkZAfb6P"},"source":["import torch.optim as opt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8cpK_wRBfjbW"},"source":["optimizer = opt.SGD(model.parameters(), lr=learning_rate) #SGD隨機update速度較快 (update對象,learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZnOc9vO7gG_x"},"source":["for epoch in range(epoch_n*100):  #下面loss是取平均過後，所以loss是上面的1/100->收斂速度變慢->epoch_n*100\n","  y_pred = model(x)\n","  loss = loss_fn(y_pred, y) #(預測結果,實際答案)\n","  if (epoch+1)%100 == 0: #每100次輸出一次loss\n","    print(\"Epoch:{}, Loss:{:.4f}\".format(epoch,loss))\n","  model.zero_grad() #計算gradient前把gradient歸零，因為linear layer內會先隨機給gradient一個initial值，所以要先清除才不會影響backward\n","  loss.backward() #會自動把可以計算gradient的都算好\n","  optimizer.step()  #更新"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"polyhIaSgctl","executionInfo":{"status":"ok","timestamp":1632989781898,"user_tz":-480,"elapsed":8,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"d3962219-d9f1-4fb2-fb89-ce281fe50a33"},"source":[" for param in model.parameters(): \n","    print(param.data)  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[3.2421, 4.8007, 1.8904]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"Y18_gvGYhkhH"},"source":["### 轉換成回歸分類問題 logistic regression\n"]},{"cell_type":"code","metadata":{"id":"C1PU6q1AhkvF"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0pyb97qghoL2"},"source":["npy = y.numpy() #轉成np array\n","bins = np.array([npy.mean()]) #mean\n","inds = np.digitize(npy, bins) #利用分割點做離散化(目標,分箱切割點) ;if(npy>bins)->1 else->0\n","cy = torch.from_numpy(inds) #類別化後的y\n","cy = cy.float() #轉型成float，不然和下面的loss function格式不同\n","cy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kAxunyG4ADdD"},"source":["### Auto-grad"]},{"cell_type":"code","metadata":{"id":"uooldShyihoG"},"source":["Vx = Variable(x, requires_grad= False)\n","Vy = Variable(cy, requires_grad= False)\n","Vw = Variable(torch.rand(in_features, out_features), requires_grad= True) #需計算gradient"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZoBSX7qfAHP3"},"source":["for epoch in range(epoch_n*1000): #收斂速度沒有很快,所以epoch*1000\n","  y_pred = torch.sigmoid(Vx.mm(Vw)) #二進制分類\n","  loss = (-1.0*(Vy*torch.log10(y_pred)\n","      +(1.0-Vy)*torch.log10(1.0-y_pred))).mean() #Cross Entropy Error\n","  if(epoch+1) % 1000 == 0:\n","    print(\"Epoch:{}, Loss:{:.4f}\".format(epoch,loss))\n","\n","  loss.backward() #會自動把可以計算gradient的都算好\n","  \n","  Vw.data -= learning_rate *Vw.grad.data  #grad放在Variable物件的.grad.data\n","\n","  Vw.grad.data.zero_()  #做loss.backword時會把grad累加進去，值會有問題->每個loop要歸零\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_cM9WPepClNp","executionInfo":{"status":"ok","timestamp":1632989801014,"user_tz":-480,"elapsed":275,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"d09c5639-dbd9-46a1-cf7e-531c8aa7da6b"},"source":["Vw  #已經把y離散化了，所以係數不是3,5,1，但係數大小關係、比例仍相同"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.1931],\n","        [ 1.2107],\n","        [-0.5965]], requires_grad=True)"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"Lz9TX_5gDX_D"},"source":["Torch.nn"]},{"cell_type":"code","metadata":{"id":"7i7j94jnDY5i"},"source":["class LogisticRegression(nn.Module):\n","  def __init__(self):\n","    super(LogisticRegression, self).__init__()\n","  def forward(self, x, w):\n","    y_pred = torch.sigmoid(x.mm(w))\n","    return y_pred\n","#backward可以從父類別繼承"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r5IDDPYIERc5"},"source":["model = LogisticRegression()\n","Vw = torch.rand(in_features, out_features, requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xgaB61ioEfSH"},"source":["for epoch in range(epoch_n*1000): #收斂速度沒有很快,所以epoch*1000\n","  y_pred = model(Vx, Vw)\n","  loss = (-1.0*(Vy*torch.log10(y_pred)\n","      +(1.0-Vy)*torch.log10(1.0-y_pred))).mean() #Cross Entropy Error\n","  if(epoch+1) % 1000 == 0:\n","    print(\"Epoch:{}, Loss:{:.4f}\".format(epoch,loss))\n","\n","  loss.backward() #會自動把可以計算gradient的都算好\n","  \n","  Vw.data -= learning_rate *Vw.grad.data  #grad放在Variable物件的.grad.data\n","\n","  Vw.grad.data.zero_()  #做loss.backword時會把grad累加進去，值會有問題->每個loop要歸零\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEsmwOqjFLJX","executionInfo":{"status":"ok","timestamp":1632989820919,"user_tz":-480,"elapsed":12,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"03aa9731-4f54-4a0e-fcd0-2eab01783578"},"source":["Vw"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.2335],\n","        [ 1.2521],\n","        [-0.6779]], requires_grad=True)"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"TeprRI1JFTKl"},"source":["###用loss function"]},{"cell_type":"code","metadata":{"id":"4jd0bJHsFYmM"},"source":["class LogisticRegression(nn.Module):\n","  def __init__(self):\n","    super(LogisticRegression, self).__init__()\n","    self.linear = nn.Linear(in_features, out_features, False) #不要bias(常數項)\n","  def forward(self, x):\n","    y_pred = torch.sigmoid(self.linear(x))\n","    return y_pred\n","#backward可以從父類別繼承"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cqiXDDNZGRxt","executionInfo":{"status":"ok","timestamp":1632989820920,"user_tz":-480,"elapsed":9,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"742c0313-9f02-48fb-bbc6-81100134776f"},"source":["model = LogisticRegression()\n","loss_fn = nn.BCELoss() #cross entropy out要是2，所以要用BCELoss(binary cross entropy)\n","print(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LogisticRegression(\n","  (linear): Linear(in_features=3, out_features=1, bias=False)\n",")\n"]}]},{"cell_type":"code","metadata":{"id":"4X_d7BqFGzFM"},"source":["for epoch in range(epoch_n*1000): #收斂速度沒有很快,所以epoch*1000\n","  y_pred = model(Vx)\n","  loss = loss_fn(y_pred, Vy) #(預測,實際)\n","  if(epoch+1) % 1000 == 0:\n","    print(\"Epoch:{}, Loss:{:.4f}\".format(epoch,loss))\n","  model.zero_grad() #backward前歸零\n","  loss.backward() #會自動把可以計算gradient的都算好\n","  for param in model.parameters():\n","    param.data -= learning_rate *param.grad.data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u_Ab6RIqILuZ","executionInfo":{"status":"ok","timestamp":1632989838175,"user_tz":-480,"elapsed":15,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"148a4596-5c85-4268-eb1a-6a838cb0bf07"},"source":["for param in model.parameters():\n","  print(param)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[ 0.3432,  1.7923, -1.3113]], requires_grad=True)\n"]}]},{"cell_type":"markdown","metadata":{"id":"jmzdfdVDRn1J"},"source":["### optimizer"]},{"cell_type":"code","metadata":{"id":"6Ao5xhQeOYov"},"source":["model = LogisticRegression()\n","optimizer = opt.SGD(model.parameters(), lr= learning_rate) #(update對象,learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LReLsU55SKu_"},"source":["for epoch in range(epoch_n*1000): #收斂速度沒有很快,所以epoch*1000\n","  y_pred = model(Vx)\n","  loss = loss_fn(y_pred, Vy) #(預測,實際)\n","  if(epoch+1) % 1000 == 0:\n","    print(\"Epoch:{}, Loss:{:.4f}\".format(epoch,loss))\n","  model.zero_grad() #backward前歸零\n","  loss.backward() #會自動把可以計算gradient的都算好\n","  optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzXDg8lLSiW-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632989858111,"user_tz":-480,"elapsed":21,"user":{"displayName":"陳宥沅","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05848884188730399773"}},"outputId":"154f8e3f-d547-4d47-86f1-55823f803853"},"source":["for param in model.parameters():\n","  print(param)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[ 0.2915,  1.7678, -1.2368]], requires_grad=True)\n"]}]}]}